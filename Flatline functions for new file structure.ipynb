{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import sqlalchemy as sql\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob('10minData/Mask*/*') # Substitute actual folder names\n",
    "def get_all_dsns(all_files):\n",
    "    '''Pickle a set of the unique dsns'''\n",
    "    my_list = set([])\n",
    "    for file in all_files:\n",
    "        my_list.add(file[:15]) # dsn should be first, double check that [:15] gives just DSN \n",
    "    \n",
    "    pickle.dump(my_list, open(f\"/Users/brodriguez/Documents/Owlet-code/{folder}/rollups_dsns.p\", \"wb\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "def find_critical_events(dsn_list, location_data, baby_info, dsns_1_bday, size_of_gap=pd.Timedelta(21,'D'), low_HR_thres=60, high_HR_thres=220, O2_thres=70, valid_thres=.4):\n",
    "    '''Find all last use cases and classify critical events'''\n",
    "    _errors = []\n",
    "    count_total = 0\n",
    "    count = 0\n",
    "    classifications = []\n",
    "    cc_dsns = pickle.load(open('cc_dsns.p', 'rb')) # dsns with connected care\n",
    "    for dsn in tqdm_notebook(dsn_list):\n",
    "        # sorted/duplicate timestamps have been dropped (rollup algorithm)\n",
    "\n",
    "        # not pickle anymore\n",
    "        df_all = pd.read_msgpack(f'TenMindata/Mask?/{dsn}.TenMinutemask.msgpack') # get details when the rollups are done\n",
    "#             df_all = pickle.load(open(f'/Volumes/baileyWD/{folder}/16000_dfs/{dsn}_df.p', 'rb')) \n",
    "        df_all = df_all.drop_duplicates()\n",
    "        at_risk_or_hardware = at_risk(df_all)\n",
    "        valid_df = df_all.loc[df_all.valid_count > 2] # > 2 so we don't miss possible cases, but also dont base critical event on 1 reading\n",
    "        dsn_location = location_data.loc[location_data.dsn == dsn] \n",
    "        dsn_baby_info = baby_info.loc[baby_info.dsn == dsn] \n",
    "\n",
    "        # df could be empty! if so, skip it\n",
    "        if valid_df.shape[0] != 0:\n",
    "            gaps = find_last_days(valid_df, baby_info.loc[baby_info.dsn == dsn],dsns_1_bday)\n",
    "            if dsn in cc_dsns:\n",
    "                cc = 1\n",
    "            else:\n",
    "                cc = 0\n",
    "            for day in gaps:\n",
    "                # Calculate age of baby and dont check for critical events if they are older than 1\n",
    "                age, bday = baby_age(day, dsn_baby_info)\n",
    "                count_total += 1\n",
    "                if (age < pd.Timedelta(days=365)) and (age >= pd.Timedelta(days=0)):\n",
    "                    count += 1\n",
    "                    # if day is w/in 2 weeks of when Tanner did the rollups - disregard. I got the data Jan 30\n",
    "                    if (str(day) <= two_weeks_before_received) & (str(day) >= '2017-01-31 00:00:00'):\n",
    "\n",
    "                        # Check if in the US\n",
    "                        if in_US(day, dsn_location):\n",
    "                            df_day = get_df_day(valid_df, day)\n",
    "                            flatline = possible_flatline(df_day, low_HR_thres, high_HR_thres, O2_thres, valid_thres)\n",
    "                            vitals = last_vitals_2(df_day) \n",
    "                            classifications.append((dsn, day.date(), flatline, vitals, cc, at_risk_or_hardware, age, bday))\n",
    "\n",
    "                elif age >= pd.Timedelta(days=365):\n",
    "                    # add row for older babies so we know the actual last day of use\n",
    "                    classifications.append((dsn, day.date(), False, 'Good vitals', cc, at_risk_or_hardware, age, bday))\n",
    "        \n",
    "    print('total babies with last day', count_total)\n",
    "    print('total babies < 1 on last day', count)\n",
    "    df_columns = ['dsn', 'date', 'critical_event', 'last_10_minutes', 'cc', 'at_risk_or_issues', 'baby_age', 'birthday']\n",
    "    df_classified = pd.DataFrame(classifications, columns=df_columns)\n",
    "    \n",
    "    # Get rid of multiple last days for individual babies\n",
    "    df_classified.sort_values(by='date', inplace=True)\n",
    "    df_classified.drop_duplicates(subset=['dsn', 'birthday'], keep='last', inplace=True)\n",
    "    \n",
    "    return df_classified\n",
    "\n",
    "\n",
    "# Base state is now a bit mask.. \n",
    "def at_risk(df):\n",
    "    '''is there at least 24 hours of base 7 data\n",
    "        If not this could be bad hardware or a baby that has a preexisting condition'''\n",
    "    # df.BaseStateMask (want all rows where the bit for base 7 is true...)\n",
    "    base_7 = df.base_state_7.cumsum()\n",
    "    mask = base_7 >= 144 \n",
    "    if df.loc[mask].shape[0] == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using big mama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle list of dsns in this round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dsns from each mask\n",
    "# combine them into 1 list\n",
    "# pickle the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need list of dsns that actually have 1st reported bday in the correct range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle inclusion dsns (the only thing not taken into account yet will be those with no data, i think..)\n",
    "\n",
    "    *Remember that we are only doing babies w/ bdays in a certain range and that are the first to use the device*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO intersect inclusions with dsns with 1st bday in correct range\n",
    "dsns_with_info = pickle.load(open('all_dsns_with_info.p','rb')) # includes cc dsns\n",
    "v2_devices = pickle.load(open('/Users/brodriguez/Documents/Owlet-code/V2_monitoring_data/v2_devices.p', 'rb'))\n",
    "inclusion_dsns = set(list_of_dsns).intersection(dsns_with_info).intersection(v2_devices)\n",
    "\n",
    "more_than_5_bdays = set(pickle.load(open(f'/Users/brodriguez/Documents/Owlet-code/{folder}/dsns_more_than_5_bdays.p', 'rb'))).intersection(inclusion_dsns)\n",
    "for dsn in more_than_5_bdays:\n",
    "    inclusion_dsns.remove(dsn)\n",
    "    \n",
    "pickle.dump(inclusion_dsns, open(f'/Users/brodriguez/Documents/Owlet-code/{folder}/inclusion_dsns.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get locations for inclusion dsns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data = pd.read_csv('/Users/brodriguez/Documents/Owlet-code/GPS_locations_Mar_2019.csv', compression='gzip')\n",
    "def dsn_in_16000(x):\n",
    "    if x['dsn'] in inclusion_dsns:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Location info for only the 16000 dsns (otherwise the df is too big and it slows everything down)\n",
    "in_16000 = location_data.apply(dsn_in_16000, axis=1)\n",
    "location_data = location_data.loc[in_16000].sort_values(by='created_at')\n",
    "\n",
    "pickle.dump(location_data, open(f'/Users/brodriguez/Documents/Owlet-code/{folder}/16000_location_data.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find critical events (load 10 min rollups with query on big mama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust to changes in accessing data\n",
    "          # changes in column names\n",
    "          # changes in column types (base state..not sure if others have changed: need to test along the way)\n",
    "# valid_count == ValidSamples\n",
    "# index is not timestamp..should I make it the index or change the code?\n",
    "def binary(x):\n",
    "    '''Convert int to binary and return indices of bits that are 1'''\n",
    "    bi = list(bin(x)[2:])[::-1]\n",
    "    indices = np.where(np.array(bi) == \"1\")[0]\n",
    "    return indices\n",
    "\n",
    "def at_risk(df):\n",
    "    if df.shape[0] < 144: \n",
    "        return True\n",
    "    else:\n",
    "        count = 0\n",
    "        for row in df.iloc[:144].itertuples():\n",
    "            if 7 in binary(int(row.BaseStateMask)):\n",
    "                count += 1\n",
    "            if count >= 144:\n",
    "                return False\n",
    "    if count >= 144:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "# find_last_days should now only return 1 day (since we only care about the first user for a given device)\n",
    "def find_last_day(dsn, df_all, dsn_bdays, dsns_1_bday):\n",
    "    ''''''\n",
    "    # if there is only 1 unique bday: return only very last day of use\n",
    "    dsn_bdays = list(dsn_bdays.sort_values(by='created_at').created_at)\n",
    "    if (dsn in dsns_1_bday) or (len(dsn_bdays) == 1):\n",
    "        return list(df_all.TimeWindowStartTime)[-1]\n",
    "    else:\n",
    "        #  take last use before second birthday was reported\n",
    "        df_all = df_all.loc[(df_all.TimeWindowStartTime >=  dsn_bdays[0]) & (df_all.TimeWindowStartTime <  dsn_bdays[1])]\n",
    "        if df_all.shape[0] == 0:\n",
    "            # No data between 1st 2 reported bdays\n",
    "            return -1\n",
    "        return list(df_all.TimeWindowStartTime)[-1]\n",
    "    \n",
    "\n",
    "def baby_age(day, baby_df, no_reg):\n",
    "    '''Use most recently reported unless you dont know when 1 was reported'''\n",
    "    # Still would need to modify the birthday data to have a column with the reported date\n",
    "    if no_reg:\n",
    "        return pd.Timedelta(days=1), ''\n",
    "    relevant_birthdays = baby_df.loc[baby_df.created_at <= str(day)] # What was reported before the last use\n",
    "    if relevant_birthdays.shape[0] == 0:\n",
    "        return pd.Timedelta(days=-1), ''\n",
    "    else:\n",
    "        # Use the last reported bday\n",
    "        last_reported = max(relevant_birthdays.created_at.values)\n",
    "        bday = relevant_birthdays.query('created_at == @last_reported').birthDate.values[0]\n",
    "    \n",
    "    age = day.date() - date(int(bday[:4]), int(bday[4:6]), int(bday[6:]))# diff between birthday and day of incident\n",
    "    return age, bday \n",
    "\n",
    "def in_US(day, location_df, no_reg):\n",
    "    '''Return True if device was used in the US'''\n",
    "    if no_reg:\n",
    "        return True\n",
    "    location = location_df.loc[location_df.created_at <= str(day)]\n",
    "    if location.shape[0] == 0:\n",
    "        # Don't know the location\n",
    "        return False\n",
    "    elif location.cc.iloc[-1] != 'US':\n",
    "        return False\n",
    "    else:\n",
    "        return True \n",
    "\n",
    "def get_df_day(df_all, day):\n",
    "    '''Get 2 hours of data just prior to the time given'''\n",
    "    # return the 2 hours just prior to the datetime (and include the datetime)\n",
    "    # if there is not data in the 2 hours before, it will get what is there\n",
    "    prior_2_hrs = day - pd.Timedelta(120,'m')\n",
    "    return df_all[(df_all.TimeWindowStartTime >= prior_2_hrs) & (df_all.TimeWindowStartTime <= day)] #.TimeWindowStartTime or .FirstReadingTime\n",
    "\n",
    "\n",
    "def possible_flatline(df, low_HR_thres, high_HR_thres, low_O2_thres, valid_thres):\n",
    "    '''Find critical events (high or low heart rate or low oxygen) in the data given'''\n",
    "    valid_percent = df.ValidSamples/df.TotalSamples\n",
    "    \n",
    "    # We don't alert for low HR unless O2 is also low.\n",
    "    critical_vitals = ((df.HeartRateRawMin < low_HR_thres) & (df.OxygenRawMin < 90)) | (df.OxygenRawMin < low_O2_thres) | (df.HeartRateRawMax > high_HR_thres)\n",
    "    critical_event = any(critical_vitals & (valid_percent >= valid_thres))\n",
    "    \n",
    "    if critical_event:\n",
    "        # Low HR won't be valid if the O2 is not actually dropping\n",
    "        if all(df.HeartRateRawMax < 220) & all(df.OxygenAvgMin >= 90):\n",
    "            # Check how many times oxygen_raw_min was below 60 \n",
    "            if (df.OxygenRawMin < 60).sum() >= 5:\n",
    "                # For cases of Oxygen Noise Index (indicating bad hardware)\n",
    "                return 'True, many instantaneous'\n",
    "            return 'True, instantaneous'\n",
    "        elif all(df.HeartRateRawMax < 220) & all(df.OxygenAvgMin < 90): \n",
    "            # (df.OxygenAvgAvg.mean() < 93) this one makes nearly everything low baseline.. \n",
    "                # meaning when they are fluctuating it still says low baseline, don't want that...\n",
    "            # this category is just a \"nice to know\"\n",
    "            return 'True, low baseline'\n",
    "        else:\n",
    "            return 'True'\n",
    "    else:\n",
    "        return 'False'\n",
    "    \n",
    "\n",
    "def last_vitals_2(df):\n",
    "    if df.shape[0] == 0:\n",
    "        return 'not valid'\n",
    "    else:\n",
    "        last_30_min = df.loc[df.ValidSamples > 0].iloc[-3:] # last 30 min or less\n",
    "        end_min_hr = last_30_min.HeartRateAvgMin.min() \n",
    "        end_max_hr = last_30_min[-2:].HeartRateAvgMax.max()# dont want 30 min\n",
    "        end_avg_o2 = last_30_min.OxygenAvgMin.min()\n",
    "        end_raw_o2 = last_30_min.OxygenRawMin.min()       \n",
    "    if (end_min_hr < 60) & (end_avg_o2 < 80): # TODO O2 threshold here may be too low\n",
    "        return 'low HR'\n",
    "    elif (end_max_hr > 220):\n",
    "        return 'high HR'\n",
    "    # if o2 < 70 make extra low o2 category?\n",
    "    elif (end_raw_o2 < 80) & (end_avg_o2 < 85): # Avg min? < 90?\n",
    "        return 'low O2'\n",
    "    else:\n",
    "        return 'Good vitals'\n",
    "    \n",
    "\n",
    "def find_critical_events(dsn_list, location_data, baby_info, dsns_1_bday, conn, low_HR_thres=60, high_HR_thres=220, O2_thres=70, valid_thres=.4, no_reg=False):\n",
    "    '''Find all last use cases and classify critical events'''\n",
    "    count_total = 0\n",
    "    count = 0\n",
    "    classifications = []\n",
    "    cc_dsns = pickle.load(open('cc_dsns.p', 'rb')) # dsns with connected care\n",
    "    for dsn in tqdm_notebook(dsn_list):\n",
    "        # sorted/duplicate timestamps have been dropped (rollup algorithm)\n",
    "\n",
    "        df_all = pd.read_sql('select * from tenminsock where dsn = %(d)s', conn, params={'d':dsn})\n",
    "        df_all = df_all.drop_duplicates()\n",
    "        df_all = df_all.sort_values(by='TimeWindowStartTime')\n",
    "        \n",
    "        at_risk_or_hardware = at_risk(df_all)\n",
    "        valid_df = df_all.loc[df_all.ValidSamples > 2] # > 2 so we don't miss possible cases, but also dont base critical event on 1 reading\n",
    "        dsn_location = location_data.loc[location_data.dsn == dsn] \n",
    "        dsn_baby_info = baby_info.loc[baby_info.dsn == dsn] \n",
    "\n",
    "        # df could be empty! if so, skip it\n",
    "        if valid_df.shape[0] != 0:\n",
    "#             gaps = find_last_days(valid_df, baby_info.loc[baby_info.dsn == dsn],dsns_1_bday)\n",
    "            last_day = find_last_day(dsn, valid_df, baby_info.loc[baby_info.dsn == dsn],dsns_1_bday)\n",
    "            if last_day != -1:\n",
    "                # there is data for 1st baby\n",
    "                if dsn in cc_dsns:\n",
    "                    cc = 1\n",
    "                else:\n",
    "                    cc = 0\n",
    "                # Calculate age of baby and dont check for critical events if they are older than 1\n",
    "                age, bday = baby_age(last_day, dsn_baby_info, no_reg)\n",
    "                count_total += 1\n",
    "                if (age < pd.Timedelta(days=365)) and (age >= pd.Timedelta(days=0)):\n",
    "                    count += 1\n",
    "                    # if day is w/in 2 weeks of last day we have data (shouldnt happen unless we need to do bdays after oct)\n",
    "    #                 if (str(day) <= two_weeks_before_received) & (str(day) >= '2017-01-31 23:59:59'):\n",
    "\n",
    "                    # Check if in the US\n",
    "                    if in_US(last_day, dsn_location, no_reg):\n",
    "                        df_day = get_df_day(valid_df, last_day)\n",
    "                        flatline = possible_flatline(df_day, low_HR_thres, high_HR_thres, O2_thres, valid_thres)\n",
    "                        vitals = last_vitals_2(df_day) \n",
    "                        classifications.append((dsn, last_day.date(), flatline, vitals, cc, at_risk_or_hardware, age, bday))\n",
    "\n",
    "                elif age >= pd.Timedelta(days=365):\n",
    "                    # add row for older babies so we know the actual last day of use\n",
    "                    classifications.append((dsn, last_day.date(), 'False', 'Good vitals', cc, at_risk_or_hardware, age, bday))\n",
    "\n",
    "    print('total babies with last day', count_total)\n",
    "    print('total babies < 1 on last day', count)\n",
    "    df_columns = ['dsn', 'date', 'critical_event', 'last_10_minutes', 'cc', 'at_risk_or_issues', 'baby_age', 'birthday']\n",
    "    df_classified = pd.DataFrame(classifications, columns=df_columns)\n",
    "    \n",
    "    # Get rid of multiple last days for individual babies\n",
    "#     df_classified.sort_values(by='date', inplace=True)\n",
    "#     df_classified.drop_duplicates(subset=['dsn', 'birthday'], keep='last', inplace=True)\n",
    "    \n",
    "    return df_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sock off classifications..How to access 2 second data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letmein\n"
     ]
    }
   ],
   "source": [
    "DB_USER = 'brodriguez'\n",
    "DB_PASSWORD = input()\n",
    "DB_CONN = 'localhost'\n",
    "DB_NAME = 'owletsock'\n",
    "# SQL login\n",
    "s = f'mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_CONN}/{DB_NAME}'\n",
    "engine = sql.create_engine(s)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '16k_round_2'\n",
    "inclusion_dsns = pickle.load(open(f'/Users/brodriguez/Documents/Owlet-code/{folder}/inclusion_dsns.p', 'rb'))\n",
    "location_data = pickle.load(open(f'/Users/brodriguez/Documents/Owlet-code/{folder}/16000_location_data.p', 'rb'))\n",
    "dsns_1_bday = pickle.load(open(f'/Users/brodriguez/Documents/Owlet-code/{folder}/dsns_1_bday.p', 'rb'))\n",
    "baby_info = pickle.load(open('16k_reg_and_cc_bdays4.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclusion_dsns = inclusion_dsns.intersection(baby_info.dsn.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04854392071549d9ab79e00c61ff7c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10889), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_classified = find_critical_events(inclusion_dsns, location_data, baby_info, dsns_1_bday, conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dsn</th>\n",
       "      <th>date</th>\n",
       "      <th>critical_event</th>\n",
       "      <th>last_10_minutes</th>\n",
       "      <th>cc</th>\n",
       "      <th>at_risk_or_issues</th>\n",
       "      <th>baby_age</th>\n",
       "      <th>birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC000W001218386</td>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>False</td>\n",
       "      <td>Good vitals</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>294 days</td>\n",
       "      <td>20180227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dsn        date critical_event last_10_minutes  cc  \\\n",
       "0  AC000W001218386  2018-12-18          False     Good vitals   1   \n",
       "\n",
       "   at_risk_or_issues baby_age  birthday  \n",
       "0               True 294 days  20180227  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once I have 2 second data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont want to apply this to whole df, just to critical events (have to get 2sec data on computer before)\n",
    "true = ['True', 'True, low baseline', 'True, instantaneous', 'True, many instantaneous']\n",
    "df_critical = df_classified.query('critical_event in @true and at_risk_or_issues == False')\n",
    "if df_critical.shape[0] != 0:\n",
    "    df_critical['signal_sock'] = df_critical.apply(signal_sock, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of classifications: ** * ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** cases \n",
    "\n",
    "df_critical[(df_critical.at_risk_or_issues == False) &\n",
    "            (df_critical.last_10_minutes != 'Good vitals') &\n",
    "            (df_critical.critical_event != 'True, instantaneous') &\n",
    "            (df_critical.critical_event != 'True, many instantaneous') &\n",
    "            (df_critical.signal_sock != 'sock off') &\n",
    "            (df_critical.signal_sock != 'data cut off')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * cases\n",
    "\n",
    "df_critical[(df_critical.at_risk_or_issues == False) &\n",
    "            (df_critical.last_10_minutes != 'Good vitals') &\n",
    "            (df_critical.critical_event != 'True, instantaneous') &\n",
    "            (df_critical.signal_sock != 'signal lost before sock off') &\n",
    "            (df_critical.signal_sock != 'unknown') &\n",
    "            (df_critical.signal_sock != 'battery died')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? cases\n",
    "\n",
    "df_critical[(df_critical.at_risk_or_issues == False) &\n",
    "            (((df_critical.last_10_minutes != 'Good vitals') &\n",
    "            ((df_critical.critical_event == 'True, instantaneous') &\n",
    "            (df_critical.signal_sock != 'sock off'))) |\n",
    "            (df_critical.critical_event == 'True, many instantaneous'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 second data still needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critical[df_critical.signal_sock == 'Need data']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
